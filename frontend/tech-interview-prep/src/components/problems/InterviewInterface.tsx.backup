"use client";

import { useState, useRef, useEffect, useCallback } from "react";
import Editor from "@monaco-editor/react";
import {
  Camera,
  CameraOff,
  Mic,
  MicOff,
  Play,
  Square,
  Send,
  Loader2,
  Wifi,
  WifiOff,
} from "lucide-react";
import { createClient } from "@/lib/supabase/client";
import { Tab, TabGroup, TabList, TabPanel, TabPanels } from "@headlessui/react";
import { useInterviewWebSocket } from "@/lib/hooks/useSocket";

interface Problem {
  id: string;
  title: string;
  description: string;
  difficulty: string;
  starter_code: Record<string, string>;
  test_cases: Array<{ input: string; expected: string; hidden: boolean }>;
}

interface Message {
  role: "user" | "assistant";
  content: string;
  timestamp?: string;
  isStreaming?: boolean;
  isSending?: boolean;
}

export default function InterviewInterface({
  problem,
  userId,
}: {
  problem: Problem;
  userId: string;
}) {
  const [language, setLanguage] = useState<"python" | "javascript">("python");
  const [code, setCode] = useState(problem.starter_code?.python || "");
  const [isRecording, setIsRecording] = useState(false);
  const [isCameraOn, setIsCameraOn] = useState(false);
  const [isMicOn, setIsMicOn] = useState(false);
  const [output, setOutput] = useState("");
  const [sessionId, setSessionId] = useState<string | null>(null);

  // AI Chat state
  const [messages, setMessages] = useState<Message[]>([]);
  const [inputMessage, setInputMessage] = useState("");
  const [isAiTyping, setIsAiTyping] = useState(false);
  const chatEndRef = useRef<HTMLDivElement>(null);

  // Push-to-talk state
  const [isPushToTalkActive, setIsPushToTalkActive] = useState(false);
  const [currentTranscription, setCurrentTranscription] = useState("");
  const transcriptionBufferRef = useRef<string[]>([]);
  const pushToTalkAudioRecorderRef = useRef<MediaRecorder | null>(null);

  // Recording refs
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioRecorderRef = useRef<MediaRecorder | null>(null);
  const recordedChunksRef = useRef<Blob[]>([]);
  const streamRef = useRef<MediaStream | null>(null);

  // Real-time processing refs
  const audioChunksRef = useRef<Blob[]>([]);
  const lastTranscriptionRef = useRef<string>("");
  const audioMimeTypeRef = useRef<string>("audio/webm");

  const supabase = createClient();

  // WebSocket connection
  const handleAIResponse = useCallback((message: string) => {
    setIsAiTyping(false);
    setMessages((prev) => [
      ...prev,
      {
        role: "assistant",
        content: message,
        timestamp: new Date().toISOString(),
      },
    ]);
  }, []);

  const handleTranscriptionEcho = useCallback((message: string) => {
    // User's transcription confirmed by server
    console.log("Transcription confirmed:", message);
  }, []);

  const {
    isConnected,
    sendTranscription,
    sendCodeUpdate,
    endInterview: wsEndInterview,
  } = useInterviewWebSocket({
    sessionId: sessionId || "temp",
    problemTitle: problem.title,
    problemDescription: problem.description,
    problemId: problem.id,
    onAIResponse: handleAIResponse,
    onTranscriptionEcho: handleTranscriptionEcho,
  });

  // Auto-scroll chat
  useEffect(() => {
    chatEndRef.current?.scrollIntoView({ behavior: "smooth" });
  }, [messages]);

  // Load starter code
  useEffect(() => {
    const starterCode = problem.starter_code?.[language] || "";
    setCode(starterCode);
  }, [language, problem]);

  // Track silence duration
  useEffect(() => {
    if (isRecording && isMicOn) {
      const interval = setInterval(() => {
        const timeSinceLastSpeech =
          (Date.now() - lastSpeechTimeRef.current) / 1000;
        setSilenceDuration(timeSinceLastSpeech);

        // Send silence event every 5 seconds if user hasn't spoken
        if (timeSinceLastSpeech >= 5 && timeSinceLastSpeech % 5 < 0.1) {
          sendTranscription("", timeSinceLastSpeech);
        }
      }, 100);

      return () => clearInterval(interval);
    }
  }, [isRecording, isMicOn, sendTranscription]);

  // Send code updates periodically
  useEffect(() => {
    if (isRecording && code) {
      const debounceTimer = setTimeout(() => {
        sendCodeUpdate(code);
      }, 2000); // Send after 2 seconds of no typing

      return () => clearTimeout(debounceTimer);
    }
  }, [code, isRecording, sendCodeUpdate]);

  // Start media stream
  const startMedia = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: isCameraOn,
        audio: isMicOn,
      });

      streamRef.current = stream;

      if (videoRef.current && isCameraOn) {
        videoRef.current.srcObject = stream;
      }

      return stream;
    } catch (error) {
      console.error("Error accessing media:", error);
      alert("Could not access camera/microphone");
      return null;
    }
  };

  const stopMedia = () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }
    if (videoRef.current) {
      videoRef.current.srcObject = null;
    }
  };

  const toggleCamera = async () => {
    if (isCameraOn) {
      if (streamRef.current) {
        const videoTrack = streamRef.current.getVideoTracks()[0];
        if (videoTrack) videoTrack.stop();
      }
      setIsCameraOn(false);
    } else {
      setIsCameraOn(true);
      if (isMicOn || isRecording) {
        stopMedia();
        await startMedia();
      } else {
        await startMedia();
      }
    }
  };

  const toggleMic = async () => {
    if (isMicOn) {
      if (streamRef.current) {
        const audioTrack = streamRef.current.getAudioTracks()[0];
        if (audioTrack) audioTrack.stop();
      }
      setIsMicOn(false);
      setIsListening(false);
    } else {
      setIsMicOn(true);
      if (isCameraOn || isRecording) {
        stopMedia();
        await startMedia();
      } else {
        await startMedia();
      }
    }
  };

  // Real-time audio transcription with WebSocket
  const startAudioTranscription = async (stream: MediaStream) => {
    if (!isMicOn) return;

    try {
      const audioStream = new MediaStream(stream.getAudioTracks());

      // Try different mime types in order of preference for OpenAI Whisper
      let mimeType = "";
      const preferredTypes = [
        "audio/mp4",
        "audio/mpeg",
        "audio/ogg",
        "audio/wav",
        "audio/webm;codecs=opus",
        "audio/webm",
      ];

      for (const type of preferredTypes) {
        if (MediaRecorder.isTypeSupported(type)) {
          mimeType = type;
          break;
        }
      }

      if (!mimeType) {
        console.error("No supported audio format found");
        return;
      }

      console.log("Using audio format:", mimeType);
      audioMimeTypeRef.current = mimeType;

      const audioRecorder = new MediaRecorder(audioStream, { mimeType });
      audioChunksRef.current = [];

      audioRecorder.ondataavailable = async (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      audioRecorder.onstop = async () => {
        // Process when stopped
        if (audioChunksRef.current.length > 0) {
          await transcribeAudioChunk();
        }
      };

      // Start recording with timeslice - this creates complete chunks with headers
      audioRecorder.start();

      // Stop and restart every 3 seconds to get complete audio files with headers
      const restartInterval = setInterval(() => {
        if (
          audioRecorderRef.current &&
          audioRecorderRef.current.state === "recording"
        ) {
          audioRecorderRef.current.stop();
          // Small delay then restart
          setTimeout(() => {
            if (audioRecorderRef.current) {
              audioRecorderRef.current.start();
            }
          }, 100);
        }
      }, 3000);

      // Store interval reference for cleanup
      (audioRecorder as any).restartInterval = restartInterval;

      audioRecorderRef.current = audioRecorder;
      setIsListening(true);
    } catch (error) {
      console.error("Audio transcription error:", error);
    }
  };

  // Transcribe audio chunk and send via WebSocket
  const transcribeAudioChunk = async () => {
    if (audioChunksRef.current.length === 0) return;

    try {
      // Create a complete audio blob from all accumulated chunks
      const audioBlob = new Blob(audioChunksRef.current, {
        type: audioMimeTypeRef.current,
      });

      // Clear chunks after creating blob
      audioChunksRef.current = [];

      // Skip very small audio files (likely silence or incomplete)
      if (audioBlob.size < 10000) {
        console.log("Skipping small audio chunk:", audioBlob.size);
        return;
      }
      const formData = new FormData();
      formData.append("audio", audioBlob, "audio.webm");

      const response = await fetch("/api/transcribe", {
        method: "POST",
        body: formData,
      });

      if (!response.ok) {
        console.error("Transcription API error");
        return;
      }

      const { text } = await response.json();

      if (text && text.trim().length > 0) {
        // Avoid duplicate transcriptions
        if (text !== lastTranscriptionRef.current) {
          lastTranscriptionRef.current = text;
          lastSpeechTimeRef.current = Date.now(); // Reset silence timer

          // Add transcribed text to buffer for streaming display
          transcriptionBufferRef.current.push(text);

          // Update streaming transcription display
          setCurrentTranscription((prev) => {
            const newText = prev ? `${prev} ${text}` : text;
            return newText;
          });

          // Send chunk to backend immediately (backend gets real-time)
          sendTranscription(text, 0);

          // Clear any existing timeout
          if (sendTimeoutRef.current) {
            clearTimeout(sendTimeoutRef.current);
          }

          // Set new timeout - after 3 seconds of silence, finalize the message
          sendTimeoutRef.current = setTimeout(() => {
            finalizeUserMessage();
          }, 3000);
        }
      }
    } catch (error) {
      console.error("Transcription error:", error);
    }
  };

  // Finalize the user message after 3 seconds of silence
  const finalizeUserMessage = () => {
    if (currentTranscription.trim()) {
      // Show the message as "sending" animation
      const userMessage: Message = {
        role: "user",
        content: currentTranscription,
        timestamp: new Date().toISOString(),
        isSending: true,
      };

      setMessages((prev) => [...prev, userMessage]);

      // Clear the streaming transcription
      setCurrentTranscription("");
      transcriptionBufferRef.current = [];

      // After brief delay, mark as sent and show AI is typing
      setTimeout(() => {
        setMessages((prev) =>
          prev.map((msg, idx) =>
            idx === prev.length - 1 ? { ...msg, isSending: false } : msg,
          ),
        );
        setIsAiTyping(true);
      }, 300);
    }
  };

  // Start interview
  const startInterview = async () => {
    if (!isCameraOn && !isMicOn) {
      alert("Please enable camera or microphone first");
      return;
    }

    if (!isConnected) {
      alert("WebSocket not connected. Please wait...");
      return;
    }

    try {
      // Create session
      const response = await fetch("/api/interviews/start", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ problemId: problem.id }),
      });
      const { session } = await response.json();
      setSessionId(session.id);

      // Start recording
      const stream = await startMedia();
      if (!stream) return;

      recordedChunksRef.current = [];

      const options = { mimeType: "video/webm;codecs=vp9,opus" };
      const mediaRecorder = new MediaRecorder(stream, options);

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          recordedChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.start();
      mediaRecorderRef.current = mediaRecorder;
      setIsRecording(true);
      lastSpeechTimeRef.current = Date.now();

      // Add introductory message when the interview starts
      setMessages((prevMessages) => [
        ...prevMessages,
        {
          role: "assistant",
          content: `Hi! Let's work on ${problem.title}. Take a moment to read the problem, and when you're ready, start by explaining your initial approach.`,
          timestamp: new Date().toISOString(),
        },
      ]);

      // Start real-time processing
      if (isMicOn) {
        await startAudioTranscription(stream);
      }
    } catch (error) {
      console.error("Start interview error:", error);
      alert("Failed to start interview");
    }
  };

  // Stop interview
  const stopInterview = async () => {
    // Stop audio transcription
    if (audioRecorderRef.current) {
      // Clear restart interval
      const restartInterval = (audioRecorderRef.current as any).restartInterval;
      if (restartInterval) {
        clearInterval(restartInterval);
      }
      audioRecorderRef.current.stop();
      audioRecorderRef.current = null;
    }
    setIsListening(false);

    // End WebSocket session
    wsEndInterview();

    // Stop main recording
    if (
      mediaRecorderRef.current &&
      mediaRecorderRef.current.state !== "inactive"
    ) {
      mediaRecorderRef.current.stop();

      await new Promise((resolve) => setTimeout(resolve, 1000));

      const blob = new Blob(recordedChunksRef.current, { type: "video/webm" });
      await saveRecording(blob);

      setIsRecording(false);
      stopMedia();
    }
  };

  // Save recording to Supabase Storage
  const saveRecording = async (blob: Blob) => {
    try {
      if (!sessionId) {
        console.error("No session ID");
        return null;
      }

      const fileName = `${userId}/${problem.id}/${sessionId}.webm`;

      const { data, error } = await supabase.storage
        .from("interview-recordings")
        .upload(fileName, blob, {
          contentType: "video/webm",
          upsert: true,
        });

      if (error) throw error;

      const { data: urlData } = supabase.storage
        .from("interview-recordings")
        .getPublicUrl(fileName);

      const videoUrl = urlData.publicUrl;

      await fetch(`/api/interviews/${sessionId}/submit`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          code,
          language,
          videoUrl,
        }),
      });

      alert("Interview submitted successfully!");
      return videoUrl;
    } catch (error) {
      console.error("Save recording error:", error);
      alert("Failed to save recording");
      return null;
    }
  };

  // Send message to AI (text fallback when mic is off)
  const sendMessage = async () => {
    if (!inputMessage.trim() || isAiTyping) return;

    const userMessage: Message = {
      role: "user",
      content: inputMessage,
      timestamp: new Date().toISOString(),
    };

    setMessages((prev) => [...prev, userMessage]);
    setInputMessage("");
    setIsAiTyping(true);

    // Send via WebSocket
    sendTranscription(inputMessage, 0);
  };

  // Run code
  const runCode = () => {
    setOutput("Code execution coming soon...\n\nYour code:\n" + code);
  };

  // Cleanup
  useEffect(() => {
    return () => {
      stopMedia();
      if (audioRecorderRef.current) {
        const restartInterval = (audioRecorderRef.current as any)
          .restartInterval;
        if (restartInterval) {
          clearInterval(restartInterval);
        }
        audioRecorderRef.current.stop();
      }
      if (sendTimeoutRef.current) {
        clearTimeout(sendTimeoutRef.current);
      }
    };
  }, []);

  return (
    <div className="flex h-full">
      <canvas ref={canvasRef} style={{ display: "none" }} />

      {/* Left Panel: Problem Description and Chat Box (Tabbed) */}
      <div className="w-1/3 border-r border-gray-200 overflow-y-auto bg-white">
        <div className="p-6">
          <TabGroup>
            <TabList className="flex justify-around mb-4 gap-2">
              <Tab className="flex-1 text-white bg-blue-600 px-4 py-2 rounded-full hover:bg-blue-700 transition focus:outline-none data-[selected]:bg-blue-800 data-[selected]:font-semibold">
                Description
              </Tab>
              <Tab className="flex-1 text-white bg-blue-600 px-4 py-2 rounded-full hover:bg-blue-700 transition focus:outline-none data-[selected]:bg-blue-800 data-[selected]:font-semibold">
                Interviewer
              </Tab>
            </TabList>

            <TabPanels>
              {/* Description Tab */}
              <TabPanel>
                <div className="flex items-center justify-between mb-4">
                  <h1 className="text-2xl font-bold text-gray-900">
                    {problem.title}
                  </h1>
                  <span
                    className={`px-3 py-1 rounded-full text-sm font-medium ${
                      problem.difficulty === "Easy"
                        ? "bg-green-100 text-green-800"
                        : problem.difficulty === "Medium"
                          ? "bg-yellow-100 text-yellow-800"
                          : "bg-red-100 text-red-800"
                    }`}
                  >
                    {problem.difficulty}
                  </span>
                </div>

                <div className="prose prose-sm max-w-none">
                  <div className="whitespace-pre-wrap text-gray-700">
                    {problem.description}
                  </div>
                </div>

                <div className="mt-6">
                  <h3 className="text-lg font-semibold mb-3 text-gray-900">
                    Test Cases
                  </h3>
                  {problem.test_cases?.map((testCase, idx) => (
                    <div
                      key={idx}
                      className="mb-4 p-4 bg-green-50 rounded-lg border border-green-200"
                    >
                      <div className="mb-2">
                        <span className="font-medium text-gray-900">
                          Input:
                        </span>
                        <code className="ml-2 text-sm bg-green-100 px-2 py-1 rounded text-gray-800">
                          {testCase.input}
                        </code>
                      </div>
                      <div>
                        <span className="font-medium text-gray-900">
                          Expected:
                        </span>
                        <code className="ml-2 text-sm bg-green-100 px-2 py-1 rounded text-gray-800">
                          {testCase.expected}
                        </code>
                      </div>
                    </div>
                  ))}
                </div>
              </TabPanel>

              {/* Interviewer Tab */}
              <TabPanel>
                <div className="flex flex-col h-[calc(100vh-200px)]">
                  <div className="p-4 border-b border-gray-200 bg-gradient-to-r from-blue-500 to-blue-600 rounded-t-lg">
                    <div className="flex items-center justify-between">
                      <div>
                        <h3 className="font-bold text-white text-lg">
                          AI Interviewer
                        </h3>
                        <p className="text-sm font-medium text-blue-100 mt-1 flex items-center gap-2">
                          {isRecording ? (
                            isListening ? (
                              <>
                                <span className="w-2 h-2 bg-green-400 rounded-full animate-pulse" />
                                Listening...{" "}
                                {silenceDuration > 0 &&
                                  `(${silenceDuration.toFixed(0)}s silence)`}
                              </>
                            ) : (
                              <>
                                <span className="w-2 h-2 bg-red-400 rounded-full animate-pulse" />
                                Interview in progress
                              </>
                            )
                          ) : (
                            "Start interview to begin"
                          )}
                        </p>
                      </div>
                      <div className="flex items-center gap-2">
                        {isConnected ? (
                          <Wifi className="text-green-300" size={20} />
                        ) : (
                          <WifiOff className="text-red-300" size={20} />
                        )}
                      </div>
                    </div>
                  </div>

                  {/* Messages */}
                  <div className="flex-1 overflow-y-auto p-4 space-y-3 bg-gray-50">
                    {messages.length === 0 && !currentTranscription ? (
                      <div className="text-center text-gray-500 mt-8">
                        <p className="text-sm">No messages yet</p>
                        <p className="text-xs mt-2">
                          Enable your camera/mic and click Start to begin!
                        </p>
                      </div>
                    ) : (
                      <>
                        {messages.map((msg, idx) => (
                          <div
                            key={idx}
                            className={`flex ${msg.role === "user" ? "justify-end" : "justify-start"} animate-fade-in`}
                          >
                            <div
                              className={`max-w-[85%] p-3 rounded-lg text-sm shadow-sm transition-all ${
                                msg.role === "user"
                                  ? msg.isSending
                                    ? "bg-blue-400 text-white rounded-br-none opacity-70 scale-95"
                                    : "bg-blue-600 text-white rounded-br-none"
                                  : "bg-white text-gray-900 border border-gray-200 rounded-bl-none"
                              }`}
                            >
                              {msg.content}
                            </div>
                          </div>
                        ))}

                        {/* Current streaming transcription */}
                        {currentTranscription && (
                          <div className="flex justify-end">
                            <div className="max-w-[85%] p-3 rounded-lg text-sm shadow-sm bg-blue-300 text-white rounded-br-none border-2 border-blue-400 border-dashed animate-pulse">
                              <span className="inline-flex items-center gap-2">
                                <Mic size={14} className="animate-pulse" />
                                {currentTranscription}
                              </span>
                            </div>
                          </div>
                        )}

                        {isAiTyping && !currentTranscription && (
                          <div className="flex justify-start">
                            <div className="bg-white border border-gray-200 p-3 rounded-lg shadow-sm">
                              <div className="flex gap-1">
                                <div
                                  className="w-2 h-2 bg-blue-600 rounded-full animate-bounce"
                                  style={{ animationDelay: "0ms" }}
                                ></div>
                                <div
                                  className="w-2 h-2 bg-blue-600 rounded-full animate-bounce"
                                  style={{ animationDelay: "150ms" }}
                                ></div>
                                <div
                                  className="w-2 h-2 bg-blue-600 rounded-full animate-bounce"
                                  style={{ animationDelay: "300ms" }}
                                ></div>
                              </div>
                            </div>
                          </div>
                        )}
                      </>
                    )}
                    <div ref={chatEndRef} />
                  </div>

                  {/* Input - Show different UI based on mic status */}
                  {isMicOn && isRecording ? (
                    <div className="p-4 border-t border-gray-200 bg-gradient-to-r from-green-50 to-blue-50">
                      <div className="text-center">
                        <div className="flex items-center justify-center gap-2 text-green-600 font-medium">
                          <Mic className="animate-pulse" size={20} />
                          <p className="text-sm">
                            Speak naturally - I am listening!
                          </p>
                        </div>
                        <p className="text-xs text-gray-500 mt-1">
                          Your voice will be transcribed automatically
                        </p>
                      </div>
                    </div>
                  ) : (
                    <div className="p-4 border-t border-gray-200 bg-white">
                      <div className="flex gap-2">
                        <input
                          type="text"
                          value={inputMessage}
                          onChange={(e) => setInputMessage(e.target.value)}
                          onKeyPress={(e) => e.key === "Enter" && sendMessage()}
                          placeholder={
                            isRecording
                              ? "Type your response..."
                              : "Start interview first"
                          }
                          disabled={!isRecording || isAiTyping}
                          className="flex-1 px-3 py-2 border border-gray-300 rounded-lg text-sm focus:outline-none focus:ring-2 focus:ring-blue-500 disabled:bg-gray-100 text-gray-900"
                        />
                        <button
                          onClick={sendMessage}
                          disabled={
                            !isRecording || !inputMessage.trim() || isAiTyping
                          }
                          className="p-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700 disabled:bg-gray-400 disabled:cursor-not-allowed transition"
                        >
                          <Send size={18} />
                        </button>
                      </div>
                    </div>
                  )}
                </div>
              </TabPanel>
            </TabPanels>
          </TabGroup>
        </div>
      </div>

      {/* Middle Panel - Code Editor */}
      <div className="flex-1 flex flex-col">
        <div className="bg-gray-800 px-4 py-2 flex items-center justify-between">
          <div className="flex gap-2">
            <button
              onClick={() => setLanguage("python")}
              className={`px-3 py-1 rounded text-sm transition ${
                language === "python"
                  ? "bg-blue-600 text-white"
                  : "bg-gray-700 text-gray-300 hover:bg-gray-600"
              }`}
            >
              Python
            </button>
            <button
              onClick={() => setLanguage("javascript")}
              className={`px-3 py-1 rounded text-sm transition ${
                language === "javascript"
                  ? "bg-blue-600 text-white"
                  : "bg-gray-700 text-gray-300 hover:bg-gray-600"
              }`}
            >
              JavaScript
            </button>
          </div>
          <div className="flex gap-2 items-center">
            {!isConnected && (
              <span className="text-xs text-red-400 flex items-center gap-1">
                <WifiOff size={14} />
                Disconnected
              </span>
            )}
            <button
              onClick={runCode}
              className="px-4 py-1 bg-green-600 hover:bg-green-700 text-white rounded text-sm transition"
            >
              Run Code
            </button>
            <button
              onClick={stopInterview}
              disabled={!isRecording}
              className="px-4 py-1 bg-blue-600 hover:bg-blue-700 text-white rounded text-sm disabled:bg-gray-600 disabled:cursor-not-allowed transition"
            >
              Submit
            </button>
          </div>
        </div>

        <div className="flex-1">
          <Editor
            height="100%"
            language={language}
            value={code}
            onChange={(value) => setCode(value || "")}
            theme="vs-dark"
            options={{
              minimap: { enabled: false },
              fontSize: 14,
              lineNumbers: "on",
              roundedSelection: false,
              scrollBeyondLastLine: false,
              automaticLayout: true,
            }}
          />
        </div>

        <div className="h-32 bg-gray-900 border-t border-gray-700 p-4 overflow-y-auto">
          <div className="text-gray-300 font-mono text-sm whitespace-pre-wrap">
            {output || "Output will appear here..."}
          </div>
        </div>
      </div>

      {/* Floating Camera Panel */}
      <div
        className="fixed bottom-6 right-6 bg-gray-800 rounded-2xl shadow-2xl p-4 z-50"
        style={{ width: "320px" }}
      >
        {isCameraOn ? (
          <div className="relative">
            <video
              ref={videoRef}
              autoPlay
              muted
              playsInline
              className="w-full h-48 object-cover rounded-lg mb-3"
              style={{ transform: "scaleX(-1)" }}
            />
            {isRecording && (
              <div className="absolute top-2 right-2 flex items-center gap-2 bg-red-600 text-white px-3 py-1 rounded-full text-xs font-semibold">
                <div className="w-2 h-2 bg-white rounded-full animate-pulse" />
                REC
              </div>
            )}
          </div>
        ) : (
          <div className="w-full h-48 flex items-center justify-center bg-gray-900 rounded-lg mb-3">
            <CameraOff size={48} className="text-gray-600" />
          </div>
        )}

        <div className="flex items-center justify-center gap-2">
          <button
            onClick={toggleCamera}
            className={`p-3 rounded-full transition ${
              isCameraOn
                ? "bg-blue-600 hover:bg-blue-700"
                : "bg-gray-700 hover:bg-gray-600"
            } text-white`}
            title={isCameraOn ? "Turn off camera" : "Turn on camera"}
          >
            {isCameraOn ? <Camera size={20} /> : <CameraOff size={20} />}
          </button>

          <button
            onClick={toggleMic}
            className={`p-3 rounded-full transition ${
              isMicOn
                ? "bg-blue-600 hover:bg-blue-700"
                : "bg-gray-700 hover:bg-gray-600"
            } text-white`}
            title={isMicOn ? "Turn off microphone" : "Turn on microphone"}
          >
            {isMicOn ? <Mic size={20} /> : <MicOff size={20} />}
          </button>

          <button
            onClick={isRecording ? stopInterview : startInterview}
            disabled={(!isCameraOn && !isMicOn && !isRecording) || !isConnected}
            className={`px-6 py-3 rounded-full font-medium text-white transition flex items-center gap-2 ${
              isRecording
                ? "bg-red-600 hover:bg-red-700 animate-pulse"
                : "bg-green-600 hover:bg-green-700 disabled:bg-gray-600 disabled:cursor-not-allowed"
            }`}
          >
            {isRecording ? (
              <>
                <Square size={16} />
                Stop
              </>
            ) : (
              <>
                <Play size={16} />
                Start
              </>
            )}
          </button>
        </div>
      </div>
    </div>
  );
}
